example_sql_to_s3:
  dag_id: example_sql_to_s3
  params: {}
  default_args:
    start_date: '2026-01-01'
  schedule: '@once'
  tasks:
    create_bucket:
      operator: airflow.providers.amazon.aws.operators.s3.S3CreateBucketOperator
      aws_conn_id: aws_default
      bucket_name: test-s3-bucket
      outlets: []
      params: {}
      priority_weight: 1
      retries: 0
      retry_delay: 300.0
      retry_exponential_backoff: false
      task_id: create_bucket
      trigger_rule: all_success
      wait_for_downstream: false
      dependencies: []
    create_cluster:
      operator: airflow.providers.amazon.aws.operators.redshift_cluster.RedshiftCreateClusterOperator
      allow_version_upgrade: true
      automated_snapshot_retention_period: 1
      aws_conn_id: aws_default
      cluster_identifier: test-redshift-cluster
      cluster_subnet_group_name: test-cluster-subnet-group
      cluster_type: single-node
      cluster_version: '1.0'
      db_name: dev
      deferrable: false
      encrypted: false
      enhanced_vpc_routing: false
      master_user_password: MyAmazonPassword1
      master_username: adminuser
      max_attempt: 5
      node_type: dc2.large
      number_of_nodes: 1
      outlets: []
      params: {}
      poll_interval: 60
      port: 5439
      priority_weight: 1
      publicly_accessible: true
      retries: 0
      retry_delay: 300.0
      retry_exponential_backoff: false
      task_id: create_cluster
      trigger_rule: all_success
      vpc_security_group_ids: [test-security-group]
      wait_for_completion: false
      wait_for_downstream: false
      dependencies: [create_bucket]
    wait_cluster_available:
      operator: airflow.providers.amazon.aws.sensors.redshift_cluster.RedshiftClusterSensor
      aws_conn_id: aws_default
      cluster_identifier: test-redshift-cluster
      deferrable: false
      mode: poke
      outlets: []
      params: {}
      poke_interval: 15
      priority_weight: 1
      retries: 0
      retry_delay: 300.0
      retry_exponential_backoff: false
      target_status: available
      task_id: wait_cluster_available
      timeout: 1800
      trigger_rule: all_success
      wait_for_downstream: false
      dependencies: [create_cluster]
    create_connection:
      outlets: []
      params: {}
      priority_weight: 1
      python_callable: airflow_sql_to_s3.create_connection
      retries: 0
      retry_delay: 300.0
      retry_exponential_backoff: false
      task_id: create_connection
      trigger_rule: all_success
      wait_for_downstream: false
      dependencies: [wait_cluster_available]
      decorator: airflow.decorators.task
      conn_id_name: test-conn-id
      cluster_id: test-redshift-cluster
    create_table_redshift_data:
      operator: airflow.providers.amazon.aws.operators.redshift_data.RedshiftDataOperator
      aws_conn_id: aws_default
      cluster_identifier: test-redshift-cluster
      database: dev
      db_user: adminuser
      deferrable: false
      outlets: []
      params: {}
      poll_interval: 10
      priority_weight: 1
      retries: 0
      retry_delay: 300.0
      retry_exponential_backoff: false
      return_sql_result: false
      sql: |2

            CREATE TABLE IF NOT EXISTS test_table (
            fruit_id INTEGER,
            name VARCHAR NOT NULL,
            color VARCHAR NOT NULL
            );
      task_id: create_table_redshift_data
      trigger_rule: all_success
      wait_for_completion: true
      wait_for_downstream: false
      with_event: false
      dependencies: [create_connection]
    insert_data:
      operator: airflow.providers.amazon.aws.operators.redshift_data.RedshiftDataOperator
      aws_conn_id: aws_default
      cluster_identifier: test-redshift-cluster
      database: dev
      db_user: adminuser
      deferrable: false
      outlets: []
      params: {}
      poll_interval: 10
      priority_weight: 1
      retries: 0
      retry_delay: 300.0
      retry_exponential_backoff: false
      return_sql_result: false
      sql: INSERT INTO test_table VALUES ( 1, 'Banana', 'Yellow');
      task_id: insert_data
      trigger_rule: all_success
      wait_for_completion: true
      wait_for_downstream: false
      with_event: false
      dependencies: [create_table_redshift_data]
    sql_to_s3_task:
      operator: airflow.providers.amazon.aws.transfers.sql_to_s3.SqlToS3Operator
      aws_conn_id: aws_default
      file_format: CSV
      groupby_kwargs: {}
      max_rows_per_file: 0
      outlets: []
      params: {}
      pd_kwargs: {}
      priority_weight: 1
      query: SELECT * FROM test_table
      replace: true
      retries: 0
      retry_delay: 300.0
      retry_exponential_backoff: false
      s3_bucket: test-s3-bucket
      s3_key: test-key
      sql_conn_id: test-conn-id
      task_id: sql_to_s3_task
      trigger_rule: all_success
      wait_for_downstream: false
      dependencies: [insert_data]
    sql_to_s3_with_groupby_task:
      operator: airflow.providers.amazon.aws.transfers.sql_to_s3.SqlToS3Operator
      aws_conn_id: aws_default
      file_format: CSV
      groupby_kwargs:
        by: color
      max_rows_per_file: 0
      outlets: []
      params: {}
      pd_kwargs: {}
      priority_weight: 1
      query: SELECT * FROM test_table
      replace: true
      retries: 0
      retry_delay: 300.0
      retry_exponential_backoff: false
      s3_bucket: test-s3-bucket
      s3_key: test-key
      sql_conn_id: test-conn-id
      task_id: sql_to_s3_with_groupby_task
      trigger_rule: all_success
      wait_for_downstream: false
      dependencies: [sql_to_s3_task]
    delete_bucket:
      operator: airflow.providers.amazon.aws.operators.s3.S3DeleteBucketOperator
      aws_conn_id: aws_default
      bucket_name: test-s3-bucket
      force_delete: true
      outlets: []
      params: {}
      priority_weight: 1
      retries: 0
      retry_delay: 300.0
      retry_exponential_backoff: false
      task_id: delete_bucket
      trigger_rule: all_done
      wait_for_downstream: false
      dependencies: [sql_to_s3_with_groupby_task]
    delete_cluster:
      operator: airflow.providers.amazon.aws.operators.redshift_cluster.RedshiftDeleteClusterOperator
      aws_conn_id: aws_default
      cluster_identifier: test-redshift-cluster
      deferrable: false
      max_attempts: 30
      outlets: []
      params: {}
      poll_interval: 30
      priority_weight: 1
      retries: 0
      retry_delay: 300.0
      retry_exponential_backoff: false
      skip_final_cluster_snapshot: true
      task_id: delete_cluster
      trigger_rule: all_done
      wait_for_completion: true
      wait_for_downstream: false
      dependencies: [delete_bucket]
